<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Machine Learning Lecture 7a Course Notes</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m45898</md:content-id>
  <md:title>Machine Learning Lecture 7a Course Notes</md:title>
  <md:abstract/>
  <md:uuid>f320c09d-1431-42d3-8bc6-7de34cd5fad8</md:uuid>
</metadata>

<content>
    <section id="id62191">
      <title>The <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means clustering algorithm</title>
      <para id="id62544">In the clustering problem, we are given a training set <m:math overflow="scroll"><m:mrow><m:mo>{</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>)</m:mo></m:mrow></m:msup><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>m</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>}</m:mo></m:mrow></m:math>,
and want to group the data into a few cohesive “clusters.”
Here, <m:math overflow="scroll"><m:mrow><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>∈</m:mo><m:msup><m:mrow><m:mi mathvariant="double-struck">R</m:mi></m:mrow><m:mi>n</m:mi></m:msup></m:mrow></m:math> as usual; but
no labels <m:math overflow="scroll"><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:math> are given. So, this is an unsupervised learning problem.</para>
      <para id="id62643">The <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means clustering algorithm is as follows:</para>
      <list id="id62655" display="block" list-type="bulleted">
        <item id="uid1"><label>1.</label>Initialize <emphasis effect="bold">cluster centroids</emphasis><m:math overflow="scroll"><m:mrow><m:msub><m:mi>μ</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>μ</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msub><m:mi>μ</m:mi><m:mi>k</m:mi></m:msub><m:mo>∈</m:mo><m:msup><m:mrow><m:mi mathvariant="double-struck">R</m:mi></m:mrow><m:mi>n</m:mi></m:msup></m:mrow></m:math> randomly.
</item>
        <item id="uid2"><label>2.</label>Repeat until convergence: <m:math overflow="scroll"><m:mo>{</m:mo></m:math><list id="id62749" display="block" list-type="bulleted"><item id="uid3"><label/>For every <m:math overflow="scroll"><m:mi>i</m:mi></m:math>, set
<equation id="id62773"><m:math overflow="scroll" mode="display"><m:mrow><m:msup><m:mi>c</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>:</m:mo><m:mo>=</m:mo><m:mo form="prefix">arg</m:mo><m:munder><m:mo movablelimits="true" form="prefix">min</m:mo><m:mi>j</m:mi></m:munder><m:mrow><m:mo>|</m:mo><m:mo>|</m:mo></m:mrow><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub><m:msup><m:mrow><m:mo>|</m:mo><m:mo>|</m:mo></m:mrow><m:mn>2</m:mn></m:msup><m:mo>.</m:mo></m:mrow></m:math></equation></item><item id="uid4"><label/>For each <m:math overflow="scroll"><m:mi>j</m:mi></m:math>, set
<equation id="id63054"><m:math overflow="scroll" mode="display"><m:mrow><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub><m:mo>:</m:mo><m:mo>=</m:mo><m:mfrac><m:mrow><m:msubsup><m:mo>∑</m:mo><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>m</m:mi></m:msubsup><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msup><m:mi>c</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>=</m:mo><m:mi>j</m:mi><m:mo>}</m:mo></m:mrow><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:mrow><m:mrow><m:msubsup><m:mo>∑</m:mo><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>m</m:mi></m:msubsup><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msup><m:mi>c</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>=</m:mo><m:mi>j</m:mi><m:mo>}</m:mo></m:mrow></m:mrow></m:mfrac><m:mo>.</m:mo></m:mrow></m:math></equation></item><item id="uid5"><label/><m:math overflow="scroll"><m:mo>}</m:mo></m:math></item></list></item>
      </list>
      <para id="id63195">In the algorithm above, <m:math overflow="scroll"><m:mi>k</m:mi></m:math> (a parameter of the algorithm) is the number of
clusters we want to find; and the cluster centroids <m:math overflow="scroll"><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub></m:math> represent our
current guesses for the positions of the centers of the clusters. To initialize the
cluster centroids (in step 1 of the algorithm above), we could choose <m:math overflow="scroll"><m:mi>k</m:mi></m:math> training
examples randomly, and set the cluster centroids to be equal to the values of
these <m:math overflow="scroll"><m:mi>k</m:mi></m:math> examples. (Other initialization methods are also possible.)</para>
      <para id="id63239">The inner-loop of the algorithm repeatedly carries out two steps: (i)
“Assigning” each training example <m:math overflow="scroll"><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:math> to the closest cluster centroid
<m:math overflow="scroll"><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub></m:math>, and (ii) Moving each cluster centroid <m:math overflow="scroll"><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub></m:math> to the mean of the
points assigned to it. <link target-id="id63304"/> shows an illustration of
running <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means.</para><para id="id63303"><figure id="id63304" orient="vertical"><subfigure id="fs-id1169772473095"><media id="id63304_media" alt="K-means algorithm. Training examples are shown as dots, and cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids (in this instance, not chosen to be equal to two training examples). (c-f) Illustration of running two iterations of k-means">
            <image mime-type="image/png" src="../../media/kmeans1.jpg" id="id63304_onlineimage" width="200"/>
            <image mime-type="application/postscript" for="pdf" src="../../media/kmeans1.eps" id="id63304_printimage"/>
          </media></subfigure><subfigure id="fs-id1169770659598">
        <media id="id63310_media" alt="">
            <image mime-type="image/png" src="../../media/kmeans2.jpg" id="id63310_onlineimage" width="200"/>
            <image mime-type="application/postscript" for="pdf" src="../../media/kmeans2.eps" id="id63310_printimage"/>
          </media></subfigure><subfigure id="fs-id1169769499689">
          <media id="id63316_media" alt="">
            <image mime-type="image/png" src="../../media/kmeans3.jpg" id="id63316_onlineimage" width="200"/>
            <image mime-type="application/postscript" for="pdf" src="../../media/kmeans3.eps" id="id63316_printimage"/>
          </media></subfigure><subfigure id="fs-id1169772450660">
          <media id="id63324_media" alt="">
            <image mime-type="image/png" src="../../media/kmeans4.jpg" id="id63324_onlineimage" width="200"/>
            <image mime-type="application/postscript" for="pdf" src="../../media/kmeans4.eps" id="id63324_printimage"/>
          </media></subfigure><subfigure id="fs-id1169773044136">
          <media id="id63330_media" alt="">
            <image mime-type="image/png" src="../../media/kmeans5.jpg" id="id63330_onlineimage" width="200"/>
            <image mime-type="application/postscript" for="pdf" src="../../media/kmeans5.eps" id="id63330_printimage"/>
          </media></subfigure><subfigure id="fs-id1169770220197">
      <media id="uid6_media" alt="">
          <image mime-type="image/png" src="../../media/kmeans6.jpg" id="uid6_onlineimage" width="200"/>
          <image mime-type="application/postscript" for="pdf" src="../../media/kmeans6.eps" id="uid6_printimage"/>
        </media></subfigure>
        
      <caption>K-means algorithm. Training examples are shown as dots, and cluster
centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster
centroids (in this instance, not chosen to be equal to two training examples). (c-f) Illustration of running
two iterations of <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means. In each iteration, we assign each training example
to the closest cluster centroid (shown by “painting” the training examples
the same color as the cluster centroid to which is assigned); then we move
each cluster centroid to the mean of the points assigned to it. (Best viewed in color.)
Images courtesy Michael Jordan.</caption></figure></para><para id="id63363">Is the <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means algorithm guaranteed to converge? Yes it is, in a certain sense.
In particular, let us define the <emphasis effect="bold">distortion function</emphasis> to be:</para>
      <equation id="id63380">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>J</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mi>c</m:mi>
              <m:mo>,</m:mo>
              <m:mi>μ</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>=</m:mo>
            <m:munderover>
              <m:mo>∑</m:mo>
              <m:mrow>
                <m:mi>i</m:mi>
                <m:mo>=</m:mo>
                <m:mn>1</m:mn>
              </m:mrow>
              <m:mi>m</m:mi>
            </m:munderover>
            <m:mrow>
              <m:mo>|</m:mo>
              <m:mo>|</m:mo>
            </m:mrow>
            <m:msup>
              <m:mi>x</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>i</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
            </m:msup>
            <m:mo>-</m:mo>
            <m:msub>
              <m:mi>μ</m:mi>
              <m:msup>
                <m:mi>c</m:mi>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>i</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:msup>
            </m:msub>
            <m:msup>
              <m:mrow>
                <m:mo>|</m:mo>
                <m:mo>|</m:mo>
              </m:mrow>
              <m:mn>2</m:mn>
            </m:msup>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id63470">Thus, <m:math overflow="scroll"><m:mi>J</m:mi></m:math> measures the sum of squared distances between each training example <m:math overflow="scroll"><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:math>
and the cluster centroid <m:math overflow="scroll"><m:msub><m:mi>μ</m:mi><m:msup><m:mi>c</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:msub></m:math> to which it has been assigned. It can
be shown that <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means is exactly coordinate descent on <m:math overflow="scroll"><m:mi>J</m:mi></m:math>. Specifically, the inner-loop
of <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means repeatedly minimizes <m:math overflow="scroll"><m:mi>J</m:mi></m:math> with respect to <m:math overflow="scroll"><m:mi>c</m:mi></m:math> while holding <m:math overflow="scroll"><m:mi>μ</m:mi></m:math> fixed, and then
minimizes <m:math overflow="scroll"><m:mi>J</m:mi></m:math> with respect to <m:math overflow="scroll"><m:mi>μ</m:mi></m:math> while holding <m:math overflow="scroll"><m:mi>c</m:mi></m:math> fixed.
Thus, <m:math overflow="scroll"><m:mi>J</m:mi></m:math> must
monotonically decrease, and the value of <m:math overflow="scroll"><m:mi>J</m:mi></m:math> must converge. (Usually, this implies
that <m:math overflow="scroll"><m:mi>c</m:mi></m:math> and <m:math overflow="scroll"><m:mi>μ</m:mi></m:math> will converge too. In theory, it is possible
for <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means to oscillate between a few different clusterings—i.e., a few different
values for <m:math overflow="scroll"><m:mi>c</m:mi></m:math> and/or <m:math overflow="scroll"><m:mi>μ</m:mi></m:math>—that have exactly
the same value of <m:math overflow="scroll"><m:mi>J</m:mi></m:math>, but this almost never happens in practice.)</para>
      <para id="id63665">The distortion function <m:math overflow="scroll"><m:mi>J</m:mi></m:math> is a non-convex function, and so coordinate
descent on <m:math overflow="scroll"><m:mi>J</m:mi></m:math> is not guaranteed to converge to the global minimum. In other words, <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means
can be susceptible to local optima. Very often <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means will work fine and come up with
very good clusterings despite this. But if you are worried about getting stuck in bad
local minima, one common thing to do is run <m:math overflow="scroll"><m:mi>k</m:mi></m:math>-means many times (using different
random initial values for the cluster centroids <m:math overflow="scroll"><m:msub><m:mi>μ</m:mi><m:mi>j</m:mi></m:msub></m:math>). Then, out of all the
different clusterings found, pick the one that gives the lowest distortion <m:math overflow="scroll"><m:mrow><m:mi>J</m:mi><m:mo>(</m:mo><m:mi>c</m:mi><m:mo>,</m:mo><m:mi>μ</m:mi><m:mo>)</m:mo></m:mrow></m:math>.</para>
    </section>
  </content>
</document>