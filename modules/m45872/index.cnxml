<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Machine Learning Lecture 4 Course Notes</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m45872</md:content-id>
  <md:title>Machine Learning Lecture 4 Course Notes</md:title>
  <md:abstract/>
  <md:uuid>077bff3a-056b-4859-a573-44a7405ff938</md:uuid>
</metadata>

<content>
    <section id="cid1">
      <title>Learning Theory</title>
      <para id="cid1_para"/>
    </section>
    <section id="cid2">
      <title>Bias/variance tradeoff</title>
      <para id="id131584">When talking about linear regression, we discussed the problem of whether
to fit a “simple” model such as the linear
“<m:math overflow="scroll"><m:mrow><m:mi>y</m:mi><m:mo>=</m:mo><m:msub><m:mi>θ</m:mi><m:mn>0</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>θ</m:mi><m:mn>1</m:mn></m:msub><m:mi>x</m:mi></m:mrow></m:math>,” or a more “complex” model such as the polynomial
“<m:math overflow="scroll"><m:mrow><m:mi>y</m:mi><m:mo>=</m:mo><m:msub><m:mi>θ</m:mi><m:mn>0</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>θ</m:mi><m:mn>1</m:mn></m:msub><m:mi>x</m:mi><m:mo>+</m:mo><m:mo>⋯</m:mo><m:msub><m:mi>θ</m:mi><m:mn>5</m:mn></m:msub><m:msup><m:mi>x</m:mi><m:mn>5</m:mn></m:msup></m:mrow></m:math>.” We saw the following example:</para>
      <para id="id131670">
        <figure id="id131672"><media id="id131672_media" alt="Data set with a best fit line going through it">
            <image mime-type="image/png" src="../../media/regressionPoly1-f3d9.png" id="id131672_onlineimage" width="485"><!-- NOTE: attribute width changes image size online (pixels). original width is 485. --></image>
            <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/regressionPoly1-3a45.eps" id="id131672_printimage"/>
          </media>
        </figure><figure id="id131678"><media id="id131678_media" alt="Data set with a better fit curve going through it">
            <image mime-type="image/png" src="../../media/regressionPoly2-42c9.png" id="id131678_onlineimage" width="485"><!-- NOTE: attribute width changes image size online (pixels). original width is 485. --></image>
            <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/regressionPoly2-76ad.eps" id="id131678_printimage"/>
          </media>
        </figure><figure id="id131684"><media id="id131684_media" alt="Data set with a best fit curve going through it touching each point along the way">
            <image mime-type="image/png" src="../../media/regressionPoly5-bd87.png" id="id131684_onlineimage" width="485"><!-- NOTE: attribute width changes image size online (pixels). original width is 485. --></image>
            <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/regressionPoly5-5d8f.eps" id="id131684_printimage"/>
          </media>
        </figure></para>
      <para id="id131690">Fitting a 5th order polynomial to the data (rightmost figure) did not result in a good
model. Specifically, even though the 5th order polynomial did a very good job
predicting <m:math overflow="scroll"><m:mi>y</m:mi></m:math> (say, prices of houses) from <m:math overflow="scroll"><m:mi>x</m:mi></m:math> (say, living area) for the examples
in the training set, we do not expect the model shown to be a good one for predicting
the prices of houses not in the training set. In other words, what's has been
learned from the training set does not <emphasis effect="italics">generalize</emphasis> well to other houses.
The <emphasis effect="bold">generalization error</emphasis> (which will be made formal shortly)
of a hypothesis is its expected error on examples not necessarily in the
training set.</para>
      <para id="id131724">Both the models in the leftmost and the rightmost figures above have large
generalization error. However, the problems that the two models suffer from
are very different. If the relationship between <m:math overflow="scroll"><m:mi>y</m:mi></m:math> and <m:math overflow="scroll"><m:mi>x</m:mi></m:math> is not linear,
then even if we were fitting a linear model to a very large amount of training
data, the linear model would still fail to accurately capture the structure
in the data. Informally, we define the <emphasis effect="bold">bias</emphasis> of a model to be the
expected generalization error even if we were to fit it to a very (say,
infinitely) large training set. Thus, for the problem above, the linear
model suffers from large bias, and may underfit (i.e., fail to capture
structure exhibited by) the data.</para>
      <para id="id131754">Apart from bias, there's a second component to the generalization error,
consisting of the <emphasis effect="bold">variance</emphasis> of a model fitting procedure. Specifically,
when fitting a 5th order polynomial as in the rightmost figure, there is a
large risk that we're fitting patterns in the data that happened to be
present in our small, finite training set, but that do not reflect the
wider pattern of the relationship between <m:math overflow="scroll"><m:mi>x</m:mi></m:math> and <m:math overflow="scroll"><m:mi>y</m:mi></m:math>. This could be,
say, because in the
training set we just happened by chance to get a slightly more-expensive-than-average
house here, and a slightly less-expensive-than-average house there, and so on.
By fitting these “spurious” patterns in the training set, we might again
obtain a model with large generalization error. In this case, we say the
model has large variance.<footnote id="uid1">In these notes, we will not try to
formalize the definitions of bias and variance beyond this discussion.
While bias and variance
are straightforward to define formally for, e.g., linear regression,
there have been several proposals for the definitions of bias and
variance for classification, and there is as yet no agreement on what is
the “right” and/or the most useful formalism.</footnote></para>
      <para id="id131803">Often, there is a tradeoff between bias and variance.
If our model is too “simple” and has very few parameters, then it may have large
bias (but small variance); if it is too “complex” and has very many parameters,
then it may suffer from large variance (but have smaller bias). In the example above,
fitting a quadratic function does better than either of the extremes of a first
or a fifth order polynomial.</para>
    </section>
    <section id="cid3">
      <title>Preliminaries</title>
      <para id="id131822">In this set of notes, we begin our foray into learning theory. Apart from being
interesting and enlightening in its own right, this discussion will also help us
hone our intuitions and derive rules of thumb about how to best apply learning algorithms
in different settings. We will also seek to answer a few questions: First, can we
make formal the bias/variance tradeoff that was just discussed?
The will also eventually lead us to talk about model selection methods, which can,
for instance, automatically decide what order polynomial to fit to a training set.
Second, in machine learning it's really generalization error
that we care about, but most learning algorithms fit their models
to the training set. Why should doing well on the training set tell us anything
about generalization error? Specifically, can we relate error on the training
set to generalization error? Third and finally, are there conditions under
which we can actually prove that learning algorithms will work well?</para>
      <para id="id131834">We start with two simple but very useful lemmas.</para>
      <para id="id131837"><emphasis effect="bold">Lemma.</emphasis> (The union bound). Let <m:math overflow="scroll"><m:mrow><m:msub><m:mi>A</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:msub><m:mi>A</m:mi><m:mn>2</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msub><m:mi>A</m:mi><m:mi>k</m:mi></m:msub></m:mrow></m:math> be <m:math overflow="scroll"><m:mi>k</m:mi></m:math> different
events (that may not be independent). Then</para>
      <equation id="id131895">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>P</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>A</m:mi>
                <m:mn>1</m:mn>
              </m:msub>
              <m:mo>∪</m:mo>
              <m:mo>⋯</m:mo>
              <m:mo>∪</m:mo>
              <m:msub>
                <m:mi>A</m:mi>
                <m:mi>k</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>≤</m:mo>
            <m:mi>P</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>A</m:mi>
                <m:mn>1</m:mn>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>+</m:mo>
            <m:mo>...</m:mo>
            <m:mo>+</m:mo>
            <m:mi>P</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>A</m:mi>
                <m:mi>k</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id132152">In probability theory, the union bound is usually stated as an axiom
(and thus we won't try to prove it), but it also makes intuitive sense: The probability of
any one of <m:math overflow="scroll"><m:mi>k</m:mi></m:math> events happening is at most the sums of the probabilities of the <m:math overflow="scroll"><m:mi>k</m:mi></m:math>
different events.</para>
      <para id="id132174"><emphasis effect="bold">Lemma.</emphasis> (Hoeffding inequality) Let <m:math overflow="scroll"><m:mrow><m:msub><m:mi>Z</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msub><m:mi>Z</m:mi><m:mi>m</m:mi></m:msub></m:mrow></m:math> be <m:math overflow="scroll"><m:mi>m</m:mi></m:math>
independent and identically distributed (iid) random variables drawn from
a Bernoulli(<m:math overflow="scroll"><m:mi>Φ</m:mi></m:math>) distribution. I.e., <m:math overflow="scroll"><m:mrow><m:mi>P</m:mi><m:mo>(</m:mo><m:msub><m:mi>Z</m:mi><m:mi>i</m:mi></m:msub><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>)</m:mo><m:mo>=</m:mo><m:mi>Φ</m:mi></m:mrow></m:math>, and <m:math overflow="scroll"><m:mrow><m:mi>P</m:mi><m:mo>(</m:mo><m:msub><m:mi>Z</m:mi><m:mi>i</m:mi></m:msub><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>)</m:mo><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>Φ</m:mi></m:mrow></m:math>.
Let <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>Φ</m:mi><m:mo>‸</m:mo></m:mover><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>/</m:mo><m:mi>m</m:mi><m:mo>)</m:mo></m:mrow><m:msubsup><m:mo>∑</m:mo><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>m</m:mi></m:msubsup><m:msub><m:mi>Z</m:mi><m:mi>i</m:mi></m:msub></m:mrow></m:math> be the mean of these random variables,
and let any <m:math overflow="scroll"><m:mrow><m:mi>γ</m:mi><m:mo>&gt;</m:mo><m:mn>0</m:mn></m:mrow></m:math> be fixed. Then</para><equation id="id132358">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mrow>
              <m:mi>P</m:mi>
              <m:mo>(</m:mo>
              <m:mo>|</m:mo>
              <m:mi>Φ</m:mi>
            </m:mrow>
            <m:mo>-</m:mo>
            <m:mover accent="true">
              <m:mi>Φ</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>|</m:mo>
              <m:mo>&gt;</m:mo>
              <m:mi>γ</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>≤</m:mo>
            <m:mn>2</m:mn>
            <m:mo form="prefix">exp</m:mo>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mo>-</m:mo>
              <m:mn>2</m:mn>
              <m:msup>
                <m:mi>γ</m:mi>
                <m:mn>2</m:mn>
              </m:msup>
              <m:mi>m</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id132425">This lemma (which in learning theory is also called the <emphasis effect="bold">Chernoff bound</emphasis>)
says that if we take <m:math overflow="scroll"><m:mover accent="true"><m:mi>Φ</m:mi><m:mo>^</m:mo></m:mover></m:math>—the average of <m:math overflow="scroll"><m:mi>m</m:mi></m:math>
Bernoulli(<m:math overflow="scroll"><m:mi>Φ</m:mi></m:math>) random variables—to be our estimate of <m:math overflow="scroll"><m:mi>Φ</m:mi></m:math>, then the
probability of our being far from the true value is small, so long as <m:math overflow="scroll"><m:mi>m</m:mi></m:math> is
large. Another way of saying this is that if you have a biased coin whose
chance of landing on heads is <m:math overflow="scroll"><m:mi>Φ</m:mi></m:math>, then if you toss it <m:math overflow="scroll"><m:mi>m</m:mi></m:math> times and
calculate the fraction of times that it came up heads, that will be a
good estimate of <m:math overflow="scroll"><m:mi>Φ</m:mi></m:math> with high probability (if <m:math overflow="scroll"><m:mi>m</m:mi></m:math> is large).</para>
      <para id="id132514">Using just these two lemmas, we will be able to prove some
of the deepest and most important results in learning theory.</para>
      <para id="id132519">To simplify our exposition, let's restrict our attention to binary
classification in which the labels are <m:math overflow="scroll"><m:mrow><m:mi>y</m:mi><m:mo>∈</m:mo><m:mo>{</m:mo><m:mn>0</m:mn><m:mo>,</m:mo><m:mn>1</m:mn><m:mo>}</m:mo></m:mrow></m:math>. Everything we'll
say here generalizes to other, including regression and multi-class
classification, problems.</para>
      <para id="id132546">We assume we are given a training set <m:math overflow="scroll"><m:mrow><m:mi>S</m:mi><m:mo>=</m:mo><m:mo>{</m:mo><m:mrow><m:mo>(</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>,</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>)</m:mo></m:mrow><m:mo>;</m:mo><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:mi>m</m:mi><m:mo>}</m:mo></m:mrow></m:math> of size <m:math overflow="scroll"><m:mi>m</m:mi></m:math>,
where the training examples <m:math overflow="scroll"><m:mrow><m:mo>(</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>,</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>)</m:mo></m:mrow></m:math> are drawn iid from
some probability distribution <m:math overflow="scroll"><m:mi mathvariant="script">D</m:mi></m:math>. For a hypothesis <m:math overflow="scroll"><m:mi>h</m:mi></m:math>, we define the
<emphasis effect="bold">training error</emphasis> (also called the <emphasis effect="bold">empirical risk</emphasis> or <emphasis effect="bold">empirical error</emphasis>
in learning theory) to be</para>
      <equation id="id132696">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mi>h</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>=</m:mo>
            <m:mfrac>
              <m:mn>1</m:mn>
              <m:mi>m</m:mi>
            </m:mfrac>
            <m:munderover>
              <m:mo>∑</m:mo>
              <m:mrow>
                <m:mi>i</m:mi>
                <m:mo>=</m:mo>
                <m:mn>1</m:mn>
              </m:mrow>
              <m:mi>m</m:mi>
            </m:munderover>
            <m:mn>1</m:mn>
            <m:mrow>
              <m:mo>{</m:mo>
              <m:mi>h</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:msup>
                  <m:mi>x</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mi>i</m:mi>
                    <m:mo>)</m:mo>
                  </m:mrow>
                </m:msup>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>≠</m:mo>
              <m:msup>
                <m:mi>y</m:mi>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>i</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:msup>
              <m:mo>}</m:mo>
            </m:mrow>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id132790">This is just the fraction of training examples that <m:math overflow="scroll"><m:mi>h</m:mi></m:math> misclassifies.
When we want to make explicit the dependence of <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> on the training set <m:math overflow="scroll"><m:mi>S</m:mi></m:math>,
we may also write this a <m:math overflow="scroll"><m:mrow><m:msub><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mi>S</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>.
We also define the generalization error to be</para>
      <equation id="id132863">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>ε</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mi>h</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>=</m:mo>
            <m:msub>
              <m:mi>P</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>,</m:mo>
                <m:mi>y</m:mi>
                <m:mo>)</m:mo>
                <m:mo>∼</m:mo>
                <m:mi mathvariant="script">D</m:mi>
              </m:mrow>
            </m:msub>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mi>h</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>≠</m:mo>
              <m:mi>y</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id132928">I.e. this is the probability that, if we now draw a new example <m:math overflow="scroll"><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow></m:math> from the distribution
<m:math overflow="scroll"><m:mi mathvariant="script">D</m:mi></m:math>, <m:math overflow="scroll"><m:mi>h</m:mi></m:math> will misclassify it.</para>
      <para id="id132969">Note that we have assumed that the training data was drawn from the <emphasis effect="italics">same</emphasis>
distribution <m:math overflow="scroll"><m:mi mathvariant="script">D</m:mi></m:math> with which we're going to evaluate our hypotheses
(in the definition of generalization error). This is
sometimes also referred to as one of the <emphasis effect="bold">PAC</emphasis> assumptions.<footnote id="uid2">PAC stands
for “probably approximately correct,” which is a framework and set of assumptions
under which numerous results on learning theory were proved. Of these, the
assumption of training and testing on the same distribution, and the assumption
of the independently drawn training examples, were the most important.</footnote></para>
      <para id="id133008">Consider the setting of linear classification, and let <m:math overflow="scroll"><m:mrow><m:msub><m:mi>h</m:mi><m:mi>θ</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msup><m:mi>θ</m:mi><m:mi>T</m:mi></m:msup><m:mi>x</m:mi><m:mo>≥</m:mo><m:mn>0</m:mn><m:mo>}</m:mo></m:mrow></m:mrow></m:math>.
What's a reasonable way of fitting the parameters <m:math overflow="scroll"><m:mi>θ</m:mi></m:math>?
One approach is to try to minimize the training error, and pick</para>
      <equation id="id133066">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mover accent="true">
              <m:mi>θ</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mo>=</m:mo>
            <m:mo form="prefix">arg</m:mo>
            <m:munder>
              <m:mo movablelimits="true" form="prefix">min</m:mo>
              <m:mi>θ</m:mi>
            </m:munder>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>h</m:mi>
                <m:mi>θ</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id133125">We call this process <emphasis effect="bold">empirical risk minimization</emphasis> (ERM), and the resulting
hypothesis output by the learning algorithm is <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>=</m:mo><m:msub><m:mi>h</m:mi><m:mover accent="true"><m:mi>θ</m:mi><m:mo>^</m:mo></m:mover></m:msub></m:mrow></m:math>.
We think of ERM as the most “basic” learning algorithm, and it will be this
algorithm that we focus on in these notes. (Algorithms such as logistic
regression can also be viewed as approximations to empirical risk minimization.)</para>
      <para id="id133171">In our study of learning theory, it will be useful to abstract away
from the specific parameterization of hypotheses and from issues such
as whether we're using a
linear classifier. We define the <emphasis effect="bold">hypothesis class</emphasis><m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> used by a learning
algorithm to be the set of all classifiers considered by it. For linear
classification, <m:math overflow="scroll"><m:mrow><m:mi mathvariant="script">H</m:mi><m:mo>=</m:mo><m:mo>{</m:mo><m:msub><m:mi>h</m:mi><m:mi>θ</m:mi></m:msub><m:mo>:</m:mo><m:msub><m:mi>h</m:mi><m:mi>θ</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msup><m:mi>θ</m:mi><m:mi>T</m:mi></m:msup><m:mi>x</m:mi><m:mo>≥</m:mo><m:mn>0</m:mn><m:mo>}</m:mo></m:mrow><m:mo>,</m:mo><m:mi>θ</m:mi><m:mo>∈</m:mo><m:msup><m:mrow><m:mi mathvariant="double-struck">R</m:mi></m:mrow><m:mrow><m:mi>n</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:msup><m:mo>}</m:mo></m:mrow></m:math>
is thus the set of all classifiers over <m:math overflow="scroll"><m:mi mathvariant="script">X</m:mi></m:math>
(the domain of the inputs)
where the decision boundary is linear.
More broadly, if we were studying, say, neural networks, then we could let <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> be
the set of all classifiers representable by some neural network architecture.</para>
      <para id="id133311">Empirical risk minimization can now be thought of as a minimization over the class
of functions <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>, in which the learning algorithm picks the hypothesis:</para>
      <equation id="id133326">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mover accent="true">
              <m:mi>h</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mo>=</m:mo>
            <m:mo form="prefix">arg</m:mo>
            <m:munder>
              <m:mo movablelimits="true" form="prefix">min</m:mo>
              <m:mrow>
                <m:mi>h</m:mi>
                <m:mo>∈</m:mo>
                <m:mi mathvariant="script">H</m:mi>
              </m:mrow>
            </m:munder>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mi>h</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
          </m:mrow>
        </m:math>
      </equation>
    </section>
    <section id="cid4">
      <title>The case of finite <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math></title>
      <para id="id133408">Let's start by considering a learning problem in which we have a finite hypothesis
class <m:math overflow="scroll"><m:mrow><m:mi mathvariant="script">H</m:mi><m:mo>=</m:mo><m:mo>{</m:mo><m:msub><m:mi>h</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msub><m:mi>h</m:mi><m:mi>k</m:mi></m:msub><m:mo>}</m:mo></m:mrow></m:math> consisting of <m:math overflow="scroll"><m:mi>k</m:mi></m:math> hypotheses. Thus, <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>
is just a set of <m:math overflow="scroll"><m:mi>k</m:mi></m:math> functions mapping from <m:math overflow="scroll"><m:mi mathvariant="script">X</m:mi></m:math>
to
<m:math overflow="scroll"><m:mrow><m:mo>{</m:mo><m:mn>0</m:mn><m:mo>,</m:mo><m:mn>1</m:mn><m:mo>}</m:mo></m:mrow></m:math>, and empirical risk minimization selects <m:math overflow="scroll"><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover></m:math> to be whichever of these <m:math overflow="scroll"><m:mi>k</m:mi></m:math> functions
has the smallest training error.</para>
      <para id="id133536">We would like to give guarantees on the generalization error of <m:math overflow="scroll"><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover></m:math>.
Our strategy for doing so will be in two parts: First, we will show that <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> is
a reliable estimate of <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:math> for all <m:math overflow="scroll"><m:mi>h</m:mi></m:math>. Second, we will show that
this implies an upper-bound on the generalization error of <m:math overflow="scroll"><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover></m:math>.</para>
      <para id="id133621">Take any one, fixed, <m:math overflow="scroll"><m:mrow><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>. Consider a Bernoulli random variable <m:math overflow="scroll"><m:mi>Z</m:mi></m:math> whose
distribution is defined as follows. We're going to sample <m:math overflow="scroll"><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo><m:mo>∼</m:mo><m:mi mathvariant="script">D</m:mi></m:mrow></m:math>.
Then, we set <m:math overflow="scroll"><m:mrow><m:mi>Z</m:mi><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>{</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>≠</m:mo><m:mi>y</m:mi><m:mo>}</m:mo></m:mrow></m:math>. I.e., we're going to draw one example,
and let <m:math overflow="scroll"><m:mi>Z</m:mi></m:math> indicate whether <m:math overflow="scroll"><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub></m:math> misclassifies it. Similarly, we also
define <m:math overflow="scroll"><m:mrow><m:msub><m:mi>Z</m:mi><m:mi>j</m:mi></m:msub><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>j</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>)</m:mo></m:mrow><m:mo>≠</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>j</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>}</m:mo></m:mrow></m:mrow></m:math>. Since our training set was drawn iid from <m:math overflow="scroll"><m:mi mathvariant="script">D</m:mi></m:math>,
<m:math overflow="scroll"><m:mi>Z</m:mi></m:math> and the <m:math overflow="scroll"><m:msub><m:mi>Z</m:mi><m:mi>j</m:mi></m:msub></m:math>'s have the same distribution.</para>
      <para id="id133846">We see that the misclassification probability on a randomly drawn
example—that is, <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:math>—is exactly the expected value
of <m:math overflow="scroll"><m:mi>Z</m:mi></m:math> (and <m:math overflow="scroll"><m:msub><m:mi>Z</m:mi><m:mi>j</m:mi></m:msub></m:math>). Moreover, the training error can be written</para>
      <equation id="id133890">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>h</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>=</m:mo>
            <m:mfrac>
              <m:mn>1</m:mn>
              <m:mi>m</m:mi>
            </m:mfrac>
            <m:munderover>
              <m:mo>∑</m:mo>
              <m:mrow>
                <m:mi>j</m:mi>
                <m:mo>=</m:mo>
                <m:mn>1</m:mn>
              </m:mrow>
              <m:mi>m</m:mi>
            </m:munderover>
            <m:msub>
              <m:mi>Z</m:mi>
              <m:mi>j</m:mi>
            </m:msub>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id133956">Thus, <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:math> is exactly the mean of the <m:math overflow="scroll"><m:mi>m</m:mi></m:math> random variables <m:math overflow="scroll"><m:msub><m:mi>Z</m:mi><m:mi>j</m:mi></m:msub></m:math> that are
drawn iid from a Bernoulli distribution with mean <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:math>.
Hence, we can apply the Hoeffding inequality, and obtain</para>
      <equation id="id134037">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mrow>
              <m:mi>P</m:mi>
              <m:mo>(</m:mo>
              <m:mo>|</m:mo>
              <m:mi>ε</m:mi>
            </m:mrow>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>h</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>-</m:mo>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>h</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mrow>
              <m:mo>|</m:mo>
              <m:mo>&gt;</m:mo>
              <m:mi>γ</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>≤</m:mo>
            <m:mn>2</m:mn>
            <m:mo form="prefix">exp</m:mo>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mo>-</m:mo>
              <m:mn>2</m:mn>
              <m:msup>
                <m:mi>γ</m:mi>
                <m:mn>2</m:mn>
              </m:msup>
              <m:mi>m</m:mi>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id134134">This shows that, for our particular <m:math overflow="scroll"><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub></m:math>, training error will be close
to generalization error with high probability, assuming <m:math overflow="scroll"><m:mi>m</m:mi></m:math> is large.
But we don't just want to guarantee that <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:math> will be close to <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:math> (with
high probability) for just only one particular <m:math overflow="scroll"><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub></m:math>. We want to prove that this will be true
for simultaneously for <emphasis effect="italics">all</emphasis><m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>. To do so, let <m:math overflow="scroll"><m:msub><m:mi>A</m:mi><m:mi>i</m:mi></m:msub></m:math> denote the event that
<m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi></m:mrow><m:mrow><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>h</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>|</m:mo><m:mo>&gt;</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math>. We've already show that, for any particular <m:math overflow="scroll"><m:msub><m:mi>A</m:mi><m:mi>i</m:mi></m:msub></m:math>,
it holds true that <m:math overflow="scroll"><m:mrow><m:mi>P</m:mi><m:mrow><m:mo>(</m:mo><m:msub><m:mi>A</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mn>2</m:mn><m:mo form="prefix">exp</m:mo><m:mrow><m:mo>(</m:mo><m:mo>-</m:mo><m:mn>2</m:mn><m:msup><m:mi>γ</m:mi><m:mn>2</m:mn></m:msup><m:mi>m</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>. Thus, using the union bound, we have
that</para>
      <equation id="id134388">
        <m:math overflow="scroll" mode="display">
          <m:mtable displaystyle="true">
            <m:mtr>
              <m:mtd columnalign="right">
                <m:mrow>
                  <m:mi>P</m:mi>
                  <m:mo>(</m:mo>
                  <m:mo>∃</m:mo>
                  <m:mspace width="0.166667em"/>
                  <m:mi>h</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi mathvariant="script">H</m:mi>
                  <m:mo>.</m:mo>
                  <m:mo>|</m:mo>
                  <m:mi>ε</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>-</m:mo>
                  <m:mover accent="true">
                    <m:mi>ε</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>|</m:mo>
                  <m:mo>&gt;</m:mo>
                  <m:mi>γ</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
              <m:mtd>
                <m:mo>=</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mi>P</m:mi>
                  <m:mo>(</m:mo>
                  <m:msub>
                    <m:mi>A</m:mi>
                    <m:mn>1</m:mn>
                  </m:msub>
                  <m:mo>∪</m:mo>
                  <m:mo>⋯</m:mo>
                  <m:mo>∪</m:mo>
                  <m:msub>
                    <m:mi>A</m:mi>
                    <m:mi>k</m:mi>
                  </m:msub>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>≤</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:munderover>
                    <m:mo>∑</m:mo>
                    <m:mrow>
                      <m:mi>i</m:mi>
                      <m:mo>=</m:mo>
                      <m:mn>1</m:mn>
                    </m:mrow>
                    <m:mi>k</m:mi>
                  </m:munderover>
                  <m:mi>P</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>A</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>≤</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:munderover>
                    <m:mo>∑</m:mo>
                    <m:mrow>
                      <m:mi>i</m:mi>
                      <m:mo>=</m:mo>
                      <m:mn>1</m:mn>
                    </m:mrow>
                    <m:mi>k</m:mi>
                  </m:munderover>
                  <m:mn>2</m:mn>
                  <m:mo form="prefix">exp</m:mo>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mo>-</m:mo>
                    <m:mn>2</m:mn>
                    <m:msup>
                      <m:mi>γ</m:mi>
                      <m:mn>2</m:mn>
                    </m:msup>
                    <m:mi>m</m:mi>
                    <m:mo>)</m:mo>
                  </m:mrow>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>=</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mn>2</m:mn>
                  <m:mi>k</m:mi>
                  <m:mo form="prefix">exp</m:mo>
                  <m:mo>(</m:mo>
                  <m:mo>-</m:mo>
                  <m:mn>2</m:mn>
                  <m:msup>
                    <m:mi>γ</m:mi>
                    <m:mn>2</m:mn>
                  </m:msup>
                  <m:mi>m</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
            </m:mtr>
          </m:mtable>
        </m:math>
      </equation>
      <para id="id134643">If we subtract both sides from 1, we find that</para>
      <equation id="id134649">
        <m:math overflow="scroll" mode="display">
          <m:mtable displaystyle="true">
            <m:mtr>
              <m:mtd columnalign="right">
                <m:mrow>
                  <m:mi>P</m:mi>
                  <m:mo>(</m:mo>
                  <m:mo>¬</m:mo>
                  <m:mo>∃</m:mo>
                  <m:mspace width="0.166667em"/>
                  <m:mi>h</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi mathvariant="script">H</m:mi>
                  <m:mo>.</m:mo>
                  <m:mo>|</m:mo>
                  <m:mi>ε</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>-</m:mo>
                  <m:mover accent="true">
                    <m:mi>ε</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>|</m:mo>
                  <m:mo>&gt;</m:mo>
                  <m:mi>γ</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
              <m:mtd>
                <m:mo>=</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mi>P</m:mi>
                  <m:mo>(</m:mo>
                  <m:mo>∀</m:mo>
                  <m:mi>h</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi mathvariant="script">H</m:mi>
                  <m:mo>.</m:mo>
                  <m:mo>|</m:mo>
                  <m:mi>ε</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>-</m:mo>
                  <m:mover accent="true">
                    <m:mi>ε</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msub>
                      <m:mi>h</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>|</m:mo>
                  <m:mo>≤</m:mo>
                  <m:mi>γ</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>≥</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mn>1</m:mn>
                  <m:mo>-</m:mo>
                  <m:mn>2</m:mn>
                  <m:mi>k</m:mi>
                  <m:mo form="prefix">exp</m:mo>
                  <m:mo>(</m:mo>
                  <m:mo>-</m:mo>
                  <m:mn>2</m:mn>
                  <m:msup>
                    <m:mi>γ</m:mi>
                    <m:mn>2</m:mn>
                  </m:msup>
                  <m:mi>m</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
            </m:mtr>
          </m:mtable>
        </m:math>
      </equation>
      <para id="id134858">(The “<m:math overflow="scroll"><m:mo>¬</m:mo></m:math>” symbol means “not.”) So, with probability
at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mn>2</m:mn><m:mi>k</m:mi><m:mo form="prefix">exp</m:mo><m:mo>(</m:mo><m:mo>-</m:mo><m:mn>2</m:mn><m:msup><m:mi>γ</m:mi><m:mn>2</m:mn></m:msup><m:mi>m</m:mi><m:mo>)</m:mo></m:mrow></m:math>, we have that
<m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:math> will be within <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> of <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>. This is
called a <emphasis effect="italics">uniform convergence</emphasis> result, because this is a bound that holds
simultaneously for all (as opposed to just one) <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>.</para>
      <para id="id135002">In the discussion above, what we did was, for particular values
of <m:math overflow="scroll"><m:mi>m</m:mi></m:math> and <m:math overflow="scroll"><m:mi>γ</m:mi></m:math>, give a bound on
the probability that for some <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>, <m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo></m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>|</m:mo><m:mo>&gt;</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math>.
There are three quantities of interest here: <m:math overflow="scroll"><m:mi>m</m:mi></m:math>, <m:math overflow="scroll"><m:mi>γ</m:mi></m:math>, and the probability of
error; we can bound either one in terms of the other two.</para>
      <para id="id135106">For instance, we can ask the following question: Given <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> and some <m:math overflow="scroll"><m:mrow><m:mi>δ</m:mi><m:mo>&gt;</m:mo><m:mn>0</m:mn></m:mrow></m:math>,
how large must <m:math overflow="scroll"><m:mi>m</m:mi></m:math> be before we can guarantee that with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, training
error will be within <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> of generalization error?
By setting <m:math overflow="scroll"><m:mrow><m:mi>δ</m:mi><m:mo>=</m:mo><m:mn>2</m:mn><m:mi>k</m:mi><m:mo form="prefix">exp</m:mo><m:mo>(</m:mo><m:mo>-</m:mo><m:mn>2</m:mn><m:msup><m:mi>γ</m:mi><m:mn>2</m:mn></m:msup><m:mi>m</m:mi><m:mo>)</m:mo></m:mrow></m:math> and solving for <m:math overflow="scroll"><m:mi>m</m:mi></m:math>,
[you should convince yourself this is the right thing to do!],
we find that if</para>
      <equation id="id135216">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>m</m:mi>
            <m:mo>≥</m:mo>
            <m:mfrac>
              <m:mn>1</m:mn>
              <m:mrow>
                <m:mn>2</m:mn>
                <m:msup>
                  <m:mi>γ</m:mi>
                  <m:mn>2</m:mn>
                </m:msup>
              </m:mrow>
            </m:mfrac>
            <m:mo form="prefix">log</m:mo>
            <m:mfrac>
              <m:mrow>
                <m:mn>2</m:mn>
                <m:mi>k</m:mi>
              </m:mrow>
              <m:mi>δ</m:mi>
            </m:mfrac>
            <m:mo>,</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id135261">then with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, we have that <m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo></m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>|</m:mo><m:mo>≤</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math>
for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>. (Equivalently, this shows that the probability that
<m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo></m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>|</m:mo><m:mo>&gt;</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math> for some <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math> is at most <m:math overflow="scroll"><m:mi>δ</m:mi></m:math>.)
This bound tells us how many training examples we need in order make
a guarantee. The training set size <m:math overflow="scroll"><m:mi>m</m:mi></m:math> that a certain method or algorithm
requires in order to achieve a certain level of performance is also called
the algorithm's <emphasis effect="bold">sample complexity</emphasis>.</para>
      <para id="id135436">The key property of the bound above is that the number of training examples
needed to make this guarantee is only <emphasis effect="italics">logarithmic</emphasis> in <m:math overflow="scroll"><m:mi>k</m:mi></m:math>,
the number of hypotheses in <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>. This will be important later.</para>
      <para id="id135465">Similarly, we can also hold <m:math overflow="scroll"><m:mi>m</m:mi></m:math> and <m:math overflow="scroll"><m:mi>δ</m:mi></m:math> fixed and solve for <m:math overflow="scroll"><m:mi>γ</m:mi></m:math>
in the previous equation,
and show [again, convince yourself that this is right!] that with probability <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, we
have that for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>,</para>
      <equation id="id135528">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mrow>
              <m:mo>|</m:mo>
            </m:mrow>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>h</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>-</m:mo>
              <m:mi>ε</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>h</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>|</m:mo>
              <m:mo>≤</m:mo>
            </m:mrow>
            <m:msqrt>
              <m:mrow>
                <m:mfrac>
                  <m:mn>1</m:mn>
                  <m:mrow>
                    <m:mn>2</m:mn>
                    <m:mi>m</m:mi>
                  </m:mrow>
                </m:mfrac>
                <m:mo form="prefix">log</m:mo>
                <m:mfrac>
                  <m:mrow>
                    <m:mn>2</m:mn>
                    <m:mi>k</m:mi>
                  </m:mrow>
                  <m:mi>δ</m:mi>
                </m:mfrac>
              </m:mrow>
            </m:msqrt>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id135603">Now, let's assume that uniform convergence holds, i.e., that
<m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo></m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>|</m:mo><m:mo>≤</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math> for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>. What can we prove about
the generalization of our learning algorithm that picked <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>=</m:mo><m:mo form="prefix">arg</m:mo><m:msub><m:mo movablelimits="true" form="prefix">min</m:mo><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:msub><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>?</para>
      <para id="id135731">Define <m:math overflow="scroll"><m:mrow><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>=</m:mo><m:mo form="prefix">arg</m:mo><m:msub><m:mo movablelimits="true" form="prefix">min</m:mo><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:msub><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> to be the best possible hypothesis in <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>.
Note that <m:math overflow="scroll"><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup></m:math> is the best that we could possibly do given that we are using <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>, so it
makes sense to compare our performance to that of <m:math overflow="scroll"><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup></m:math>. We have:</para>
      <equation id="id135839">
        <m:math overflow="scroll" mode="display">
          <m:mtable displaystyle="true">
            <m:mtr>
              <m:mtd columnalign="right">
                <m:mrow>
                  <m:mi>ε</m:mi>
                  <m:mo>(</m:mo>
                  <m:mover accent="true">
                    <m:mi>h</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mtd>
              <m:mtd>
                <m:mo>≤</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mover accent="true">
                    <m:mi>ε</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mover accent="true">
                      <m:mi>h</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>+</m:mo>
                  <m:mi>γ</m:mi>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>≤</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mover accent="true">
                    <m:mi>ε</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msup>
                      <m:mi>h</m:mi>
                      <m:mo>*</m:mo>
                    </m:msup>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>+</m:mo>
                  <m:mi>γ</m:mi>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>≤</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mi>ε</m:mi>
                  <m:mo>(</m:mo>
                  <m:msup>
                    <m:mi>h</m:mi>
                    <m:mo>*</m:mo>
                  </m:msup>
                  <m:mo>)</m:mo>
                  <m:mo>+</m:mo>
                  <m:mn>2</m:mn>
                  <m:mi>γ</m:mi>
                </m:mrow>
              </m:mtd>
            </m:mtr>
          </m:mtable>
        </m:math>
      </equation>
      <para id="id135975">The first line used the fact that <m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi></m:mrow><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mrow><m:mo>|</m:mo><m:mo>≤</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math> (by our uniform
convergence assumption). The second used the fact that <m:math overflow="scroll"><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover></m:math> was chosen to minimize <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>,
and hence <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> for all <m:math overflow="scroll"><m:mi>h</m:mi></m:math>, and in particular
<m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow></m:mrow></m:math>. The third line used the uniform convergence assumption again,
to show that <m:math overflow="scroll"><m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mi>γ</m:mi></m:mrow></m:math>. So, what
we've shown is the following: If uniform convergence occurs,
then the generalization error of <m:math overflow="scroll"><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover></m:math> is at
most <m:math overflow="scroll"><m:mrow><m:mn>2</m:mn><m:mi>γ</m:mi></m:mrow></m:math> worse than the best possible hypothesis in <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>!</para>
      <para id="id136291">Let's put all this together into a theorem.</para>
      <para id="id136295"><emphasis effect="bold">Theorem.</emphasis> Let <m:math overflow="scroll"><m:mrow><m:mo>|</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>|</m:mo><m:mo>=</m:mo><m:mi>k</m:mi></m:mrow></m:math>, and let any <m:math overflow="scroll"><m:mrow><m:mi>m</m:mi><m:mo>,</m:mo><m:mi>δ</m:mi></m:mrow></m:math> be fixed. Then with probability
at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, we have that</para>
      <equation id="id136357">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>ε</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mover accent="true">
                <m:mi>h</m:mi>
                <m:mo>^</m:mo>
              </m:mover>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>≤</m:mo>
            <m:mfenced separators="" open="(" close=")">
              <m:munder>
                <m:mo movablelimits="true" form="prefix">min</m:mo>
                <m:mrow>
                  <m:mi>h</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi mathvariant="script">H</m:mi>
                </m:mrow>
              </m:munder>
              <m:mi>ε</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>h</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
            </m:mfenced>
            <m:mo>+</m:mo>
            <m:mn>2</m:mn>
            <m:msqrt>
              <m:mrow>
                <m:mfrac>
                  <m:mn>1</m:mn>
                  <m:mrow>
                    <m:mn>2</m:mn>
                    <m:mi>m</m:mi>
                  </m:mrow>
                </m:mfrac>
                <m:mo form="prefix">log</m:mo>
                <m:mfrac>
                  <m:mrow>
                    <m:mn>2</m:mn>
                    <m:mi>k</m:mi>
                  </m:mrow>
                  <m:mi>δ</m:mi>
                </m:mfrac>
              </m:mrow>
            </m:msqrt>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id136455">This is proved by letting <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> equal the <m:math overflow="scroll"><m:msqrt><m:mo>·</m:mo></m:msqrt></m:math> term, using our previous argument
that uniform convergence occurs with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, and
then noting that uniform convergence implies <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:math> is at
most <m:math overflow="scroll"><m:mrow><m:mn>2</m:mn><m:mi>γ</m:mi></m:mrow></m:math> higher than <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:msub><m:mo movablelimits="true" form="prefix">min</m:mo><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:msub><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>
(as we showed previously).</para>
      <para id="id136581">This also quantifies what we were saying previously saying about the bias/variance tradeoff in model
selection. Specifically, suppose we have some hypothesis class <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>, and are
considering switching to some much larger hypothesis class <m:math overflow="scroll"><m:mrow><m:msup><m:mrow><m:mi mathvariant="script">H</m:mi></m:mrow><m:mo>'</m:mo></m:msup><m:mo>⊇</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>.
If we switch to <m:math overflow="scroll"><m:msup><m:mrow><m:mi mathvariant="script">H</m:mi></m:mrow><m:mo>'</m:mo></m:msup></m:math>, then the first term
<m:math overflow="scroll"><m:mrow><m:msub><m:mo movablelimits="true" form="prefix">min</m:mo><m:mi>h</m:mi></m:msub><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> can only decrease (since we'd then be taking a min
over a larger set of functions). Hence,
by learning using a larger hypothesis class, our “bias” can only
decrease. However, if k increases, then the second <m:math overflow="scroll"><m:mrow><m:mn>2</m:mn><m:msqrt><m:mo>·</m:mo></m:msqrt></m:mrow></m:math> term
would also increase. This increase corresponds to our “variance” increasing
when we use a larger hypothesis class.</para>
      <para id="id136690">By holding <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> and <m:math overflow="scroll"><m:mi>δ</m:mi></m:math> fixed and solving for <m:math overflow="scroll"><m:mi>m</m:mi></m:math> like we
did before, we can also obtain the following sample complexity bound:</para>
      <para id="id136721"><emphasis effect="bold">Corollary.</emphasis> Let <m:math overflow="scroll"><m:mrow><m:mo>|</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>|</m:mo><m:mo>=</m:mo><m:mi>k</m:mi></m:mrow></m:math>, and let any <m:math overflow="scroll"><m:mrow><m:mi>δ</m:mi><m:mo>,</m:mo><m:mi>γ</m:mi></m:mrow></m:math> be fixed. Then for
<m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:msub><m:mo movablelimits="true" form="prefix">min</m:mo><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:msub><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mn>2</m:mn><m:mi>γ</m:mi></m:mrow></m:math> to hold with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>,
it suffices that</para>
      <equation id="id136846">
        <m:math overflow="scroll" mode="display">
          <m:mtable displaystyle="true">
            <m:mtr>
              <m:mtd columnalign="right">
                <m:mi>m</m:mi>
              </m:mtd>
              <m:mtd>
                <m:mo>≥</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mfrac>
                    <m:mn>1</m:mn>
                    <m:mrow>
                      <m:mn>2</m:mn>
                      <m:msup>
                        <m:mi>γ</m:mi>
                        <m:mn>2</m:mn>
                      </m:msup>
                    </m:mrow>
                  </m:mfrac>
                  <m:mo form="prefix">log</m:mo>
                  <m:mfrac>
                    <m:mrow>
                      <m:mn>2</m:mn>
                      <m:mi>k</m:mi>
                    </m:mrow>
                    <m:mi>δ</m:mi>
                  </m:mfrac>
                </m:mrow>
              </m:mtd>
            </m:mtr>
            <m:mtr>
              <m:mtd/>
              <m:mtd>
                <m:mo>=</m:mo>
              </m:mtd>
              <m:mtd columnalign="left">
                <m:mrow>
                  <m:mi>O</m:mi>
                  <m:mfenced separators="" open="(" close=")">
                    <m:mfrac>
                      <m:mn>1</m:mn>
                      <m:msup>
                        <m:mi>γ</m:mi>
                        <m:mn>2</m:mn>
                      </m:msup>
                    </m:mfrac>
                    <m:mo form="prefix">log</m:mo>
                    <m:mfrac>
                      <m:mi>k</m:mi>
                      <m:mi>δ</m:mi>
                    </m:mfrac>
                  </m:mfenced>
                  <m:mo>,</m:mo>
                </m:mrow>
              </m:mtd>
            </m:mtr>
          </m:mtable>
        </m:math>
      </equation>
    </section>
    <section id="cid5">
      <title>The case of infinite <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math></title>
      <para id="id136967">We have proved some useful theorems for the case of finite hypothesis
classes. But many hypothesis classes, including any parameterized by real numbers
(as in linear classification) actually contain an infinite number of functions.
Can we prove similar results for this setting?</para>
      <para id="id136972">Let's start by going through something that is <emphasis effect="italics">not</emphasis> the “right” argument.
<emphasis effect="italics">Better and more general arguments exist</emphasis>, but this will be useful for
honing our intuitions about the domain.</para>
      <para id="id136988">Suppose we have an <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> that is parameterized by <m:math overflow="scroll"><m:mi>d</m:mi></m:math> real numbers. Since we are using a computer to
represent real numbers, and IEEE double-precision floating point (<code display="inline">double</code>'s in C)
uses 64 bits
to represent a floating point number, this means that our learning algorithm, assuming we're using
double-precision floating point, is parameterized by <m:math overflow="scroll"><m:mrow><m:mn>64</m:mn><m:mi>d</m:mi></m:mrow></m:math> bits. Thus, our hypothesis class
really consists of at most <m:math overflow="scroll"><m:mrow><m:mi>k</m:mi><m:mo>=</m:mo><m:msup><m:mn>2</m:mn><m:mrow><m:mn>64</m:mn><m:mi>d</m:mi></m:mrow></m:msup></m:mrow></m:math> different hypotheses. From the Corollary at the end of the
previous section, we therefore find that, to guarantee
<m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mn>2</m:mn><m:mi>γ</m:mi></m:mrow></m:math>, with to hold with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>,
it suffices that
<m:math overflow="scroll"><m:mrow><m:mi>m</m:mi><m:mo>≥</m:mo><m:mi>O</m:mi><m:mfenced separators="" open="(" close=")"><m:mfrac><m:mn>1</m:mn><m:msup><m:mi>γ</m:mi><m:mn>2</m:mn></m:msup></m:mfrac><m:mo form="prefix">log</m:mo><m:mfrac><m:msup><m:mn>2</m:mn><m:mrow><m:mn>64</m:mn><m:mi>d</m:mi></m:mrow></m:msup><m:mi>δ</m:mi></m:mfrac></m:mfenced><m:mo>=</m:mo><m:mi>O</m:mi><m:mfenced separators="" open="(" close=")"><m:mfrac><m:mi>d</m:mi><m:msup><m:mi>γ</m:mi><m:mn>2</m:mn></m:msup></m:mfrac><m:mo form="prefix">log</m:mo><m:mfrac><m:mn>1</m:mn><m:mi>δ</m:mi></m:mfrac></m:mfenced><m:mo>=</m:mo><m:msub><m:mi>O</m:mi><m:mrow><m:mi>γ</m:mi><m:mo>,</m:mo><m:mi>δ</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>d</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>. (The <m:math overflow="scroll"><m:mrow><m:mi>γ</m:mi><m:mo>,</m:mo><m:mi>δ</m:mi></m:mrow></m:math> subscripts are to indicate that the last big-<m:math overflow="scroll"><m:mi>O</m:mi></m:math> is hiding constants that may
depend on <m:math overflow="scroll"><m:mi>γ</m:mi></m:math> and <m:math overflow="scroll"><m:mi>δ</m:mi></m:math>.) Thus, the number of training examples needed is at most <emphasis effect="italics">linear</emphasis>
in the parameters of the model.</para>
      <para id="id137278">The fact that we relied on 64-bit floating point makes this argument not entirely satisfying,
but the conclusion is nonetheless roughly correct: If what we're going to do is try to minimize training error,
then in order to learn “well” using a hypothesis class that has <m:math overflow="scroll"><m:mi>d</m:mi></m:math> parameters, generally we're
going to need on the order of a linear number of training examples in <m:math overflow="scroll"><m:mi>d</m:mi></m:math>.</para>
      <para id="id137304">(At this point, it's worth noting that these results were proved for an algorithm
that uses empirical risk minimization. Thus, while the linear dependence of sample
complexity on <m:math overflow="scroll"><m:mi>d</m:mi></m:math> does generally
hold for most discriminative learning algorithms that try to minimize training
error or some approximation to training error, these conclusions do not
always apply as readily to discriminative learning algorithms. Giving good theoretical
guarantees on many non-ERM learning algorithms is still an area of
active research.)</para>
      <para id="id137321">The other part of our previous argument that's slightly unsatisfying is that
it relies on the parameterization of <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>.
Intuitively, this doesn't seem like it should matter: We had written the class
of linear classifiers as
<m:math overflow="scroll"><m:mrow><m:msub><m:mi>h</m:mi><m:mi>θ</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msub><m:mi>θ</m:mi><m:mn>0</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>θ</m:mi><m:mn>1</m:mn></m:msub><m:msub><m:mi>x</m:mi><m:mn>1</m:mn></m:msub><m:mo>+</m:mo><m:mo>⋯</m:mo><m:msub><m:mi>θ</m:mi><m:mi>n</m:mi></m:msub><m:msub><m:mi>x</m:mi><m:mi>n</m:mi></m:msub><m:mo>≥</m:mo><m:mn>0</m:mn><m:mo>}</m:mo></m:mrow></m:mrow></m:math>,
with <m:math overflow="scroll"><m:mrow><m:mi>n</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:math> parameters <m:math overflow="scroll"><m:mrow><m:msub><m:mi>θ</m:mi><m:mn>0</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msub><m:mi>θ</m:mi><m:mi>n</m:mi></m:msub></m:mrow></m:math>.
But it could also be written
<m:math overflow="scroll"><m:mrow><m:msub><m:mi>h</m:mi><m:mrow><m:mi>u</m:mi><m:mo>,</m:mo><m:mi>v</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:mrow><m:mo>(</m:mo><m:msubsup><m:mi>u</m:mi><m:mn>0</m:mn><m:mn>2</m:mn></m:msubsup><m:mo>-</m:mo><m:msubsup><m:mi>v</m:mi><m:mn>0</m:mn><m:mn>2</m:mn></m:msubsup><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mrow><m:mo>(</m:mo><m:msubsup><m:mi>u</m:mi><m:mn>1</m:mn><m:mn>2</m:mn></m:msubsup><m:mo>-</m:mo><m:msubsup><m:mi>v</m:mi><m:mn>1</m:mn><m:mn>2</m:mn></m:msubsup><m:mo>)</m:mo></m:mrow><m:msub><m:mi>x</m:mi><m:mn>1</m:mn></m:msub><m:mo>+</m:mo><m:mo>⋯</m:mo><m:mrow><m:mo>(</m:mo><m:msubsup><m:mi>u</m:mi><m:mi>n</m:mi><m:mn>2</m:mn></m:msubsup><m:mo>-</m:mo><m:msubsup><m:mi>v</m:mi><m:mi>n</m:mi><m:mn>2</m:mn></m:msubsup><m:mo>)</m:mo></m:mrow><m:msub><m:mi>x</m:mi><m:mi>n</m:mi></m:msub><m:mo>≥</m:mo><m:mn>0</m:mn><m:mo>}</m:mo></m:mrow></m:mrow></m:math> with <m:math overflow="scroll"><m:mrow><m:mn>2</m:mn><m:mi>n</m:mi><m:mo>+</m:mo><m:mn>2</m:mn></m:mrow></m:math> parameters <m:math overflow="scroll"><m:mrow><m:msub><m:mi>u</m:mi><m:mi>i</m:mi></m:msub><m:mo>,</m:mo><m:msub><m:mi>v</m:mi><m:mi>i</m:mi></m:msub></m:mrow></m:math>.
Yet, both of these are just defining the same <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>: The set of linear classifiers in <m:math overflow="scroll"><m:mi>n</m:mi></m:math> dimensions.</para>
      <para id="id137675">To derive a more satisfying argument, let's define a few more things.</para>
      <para id="id137679">Given a set <m:math overflow="scroll"><m:mrow><m:mi>S</m:mi><m:mo>=</m:mo><m:mo>{</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>d</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>}</m:mo></m:mrow></m:math> (no relation to the training set)
of points <m:math overflow="scroll"><m:mrow><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>∈</m:mo><m:mi mathvariant="script">X</m:mi></m:mrow></m:math>, we say that <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math><emphasis effect="bold">shatters</emphasis><m:math overflow="scroll"><m:mi>S</m:mi></m:math> if <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> can realize any
labeling on <m:math overflow="scroll"><m:mi>S</m:mi></m:math>. I.e., if for any set of labels <m:math overflow="scroll"><m:mrow><m:mo>{</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>)</m:mo></m:mrow></m:msup><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>d</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>}</m:mo></m:mrow></m:math>,
there exists
some <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math> so that <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>x</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:msup><m:mi>y</m:mi><m:mrow><m:mo>(</m:mo><m:mi>i</m:mi><m:mo>)</m:mo></m:mrow></m:msup></m:mrow></m:math> for all <m:math overflow="scroll"><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>,</m:mo><m:mo>...</m:mo><m:mi>d</m:mi></m:mrow></m:math>.</para>
      <para id="id137929">Given a hypothesis class <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>, we then define its <emphasis effect="bold">Vapnik-Chervonenkis dimension</emphasis>,
written <m:math overflow="scroll"><m:mrow><m:mi> VC </m:mi><m:mo>(</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>)</m:mo></m:mrow></m:math>, to be the
size of the largest set that is shattered by <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>. (If <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> can shatter arbitrarily large sets, then
<m:math overflow="scroll"><m:mrow><m:mi> VC </m:mi><m:mo>(</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>)</m:mo><m:mo>=</m:mo><m:mi>∞</m:mi></m:mrow></m:math>.)</para>
      <para id="id138014">For instance, consider the following set of three points:</para>
      <figure id="uid3"><media id="uid3_media" alt="three points on a graph">
          <image mime-type="image/png" src="../../media/shatter3.png" id="uid3_onlineimage" width="119"><!-- NOTE: attribute width changes image size online (pixels). original width is 119. --></image>
          <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/shatter3.eps" id="uid3_printimage"/>
        </media>
      </figure><para id="id138026">Can the set <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> of linear classifiers in two
dimensions (<m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mn>1</m:mn><m:mrow><m:mo>{</m:mo><m:msub><m:mi>θ</m:mi><m:mn>0</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>θ</m:mi><m:mn>1</m:mn></m:msub><m:msub><m:mi>x</m:mi><m:mn>1</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>θ</m:mi><m:mn>2</m:mn></m:msub><m:msub><m:mi>x</m:mi><m:mn>2</m:mn></m:msub><m:mo>≥</m:mo><m:mn>0</m:mn><m:mo>}</m:mo></m:mrow></m:mrow></m:math>) can shatter
the set above? The answer is yes. Specifically, we see that, for any of the eight possible
labelings of these points, we can find a linear classifier that obtains “zero training error”
on them:</para>
      <figure id="uid4"><media id="uid4_media" alt="drawing different lines to break the three data points into different groups">
          <image mime-type="image/png" src="../../media/shatter3-8.png" id="uid4_onlineimage" width="551"><!-- NOTE: attribute width changes image size online (pixels). original width is 551. --></image>
          <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/shatter3-8.eps" id="uid4_printimage"/>
        </media>
      </figure><para id="id138127">Moreover, it is possible to show that there is no set of 4 points that this
hypothesis class can shatter. Thus, the largest set that <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> can shatter
is of size 3, and hence <m:math overflow="scroll"><m:mrow><m:mi> VC </m:mi><m:mo>(</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>)</m:mo><m:mo>=</m:mo><m:mn>3</m:mn></m:mrow></m:math>.</para>
      <para id="id138166">Note that the VC dimension of <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> here is 3 even though there may be sets of
size 3 that it cannot shatter. For instance, if we had a set of three points
lying in a straight line (left figure), then there is no way to find a
linear separator for the labeling of the three points shown below (right figure):</para>
      <figure id="uid5"><media id="uid5_media" alt="three points in a line, second is broken up into different groups">
          <image mime-type="image/png" src="../../media/collinear-notshatter.png" id="uid5_onlineimage" width="299"><!-- NOTE: attribute width changes image size online (pixels). original width is 299. --></image>
          <image mime-type="application/postscript" for="pdf" print-width="3.5in" src="../../media/collinear-notshatter.eps" id="uid5_printimage"/>
        </media>
      </figure><para id="id138191">In order words, under the definition of the VC dimension, in order to prove that
<m:math overflow="scroll"><m:mrow><m:mi> VC </m:mi><m:mo>(</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>)</m:mo></m:mrow></m:math> is at least <m:math overflow="scroll"><m:mi>d</m:mi></m:math>, we need to show only
that there's at least <emphasis effect="italics">one</emphasis> set of size <m:math overflow="scroll"><m:mi>d</m:mi></m:math> that <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> can shatter.</para>
      <para id="id138249">The following theorem, due to Vapnik, can then be shown. (This is, many would argue,
the most important theorem in all of learning theory.)</para>
      <para id="id138253"><emphasis effect="bold">Theorem.</emphasis>
Let <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> be given, and let <m:math overflow="scroll"><m:mrow><m:mi>d</m:mi><m:mo>=</m:mo><m:mi> VC </m:mi><m:mo>(</m:mo><m:mi mathvariant="script">H</m:mi><m:mo>)</m:mo></m:mrow></m:math>. Then with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, we
have that for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>,</para>
      <equation id="id138329">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mrow>
              <m:mo>|</m:mo>
              <m:mi>ε</m:mi>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>h</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>-</m:mo>
            </m:mrow>
            <m:mover accent="true">
              <m:mi>ε</m:mi>
              <m:mo>^</m:mo>
            </m:mover>
            <m:mrow>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>h</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>|</m:mo>
              <m:mo>≤</m:mo>
              <m:mi>O</m:mi>
            </m:mrow>
            <m:mfenced open="(" close=")">
              <m:msqrt>
                <m:mrow>
                  <m:mfrac>
                    <m:mi>d</m:mi>
                    <m:mi>m</m:mi>
                  </m:mfrac>
                  <m:mo form="prefix">log</m:mo>
                  <m:mfrac>
                    <m:mi>m</m:mi>
                    <m:mi>d</m:mi>
                  </m:mfrac>
                  <m:mo>+</m:mo>
                  <m:mfrac>
                    <m:mn>1</m:mn>
                    <m:mi>m</m:mi>
                  </m:mfrac>
                  <m:mo form="prefix">log</m:mo>
                  <m:mfrac>
                    <m:mn>1</m:mn>
                    <m:mi>δ</m:mi>
                  </m:mfrac>
                </m:mrow>
              </m:msqrt>
            </m:mfenced>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id138424">Thus, with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>, we also have that:</para>
      <equation id="id138444">
        <m:math overflow="scroll" mode="display">
          <m:mrow>
            <m:mi>ε</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:mover accent="true">
                <m:mi>h</m:mi>
                <m:mo>^</m:mo>
              </m:mover>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>≤</m:mo>
            <m:mi>ε</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msup>
                <m:mi>h</m:mi>
                <m:mo>*</m:mo>
              </m:msup>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>+</m:mo>
            <m:mi>O</m:mi>
            <m:mfenced open="(" close=")">
              <m:msqrt>
                <m:mrow>
                  <m:mfrac>
                    <m:mi>d</m:mi>
                    <m:mi>m</m:mi>
                  </m:mfrac>
                  <m:mo form="prefix">log</m:mo>
                  <m:mfrac>
                    <m:mi>m</m:mi>
                    <m:mi>d</m:mi>
                  </m:mfrac>
                  <m:mo>+</m:mo>
                  <m:mfrac>
                    <m:mn>1</m:mn>
                    <m:mi>m</m:mi>
                  </m:mfrac>
                  <m:mo form="prefix">log</m:mo>
                  <m:mfrac>
                    <m:mn>1</m:mn>
                    <m:mi>δ</m:mi>
                  </m:mfrac>
                </m:mrow>
              </m:msqrt>
            </m:mfenced>
            <m:mo>.</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id138538">In other words, if a hypothesis class has finite VC dimension,
then uniform convergence occurs as <m:math overflow="scroll"><m:mi>m</m:mi></m:math> becomes large.
As before,
this allows us to give a bound on <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow></m:math> in terms
of <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow></m:math>. We also
have the following corollary:</para>
      <para id="id138591"><emphasis effect="bold">Corollary.</emphasis> For <m:math overflow="scroll"><m:mrow><m:mrow><m:mo>|</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>-</m:mo></m:mrow><m:mover accent="true"><m:mi>ε</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mrow><m:mo>(</m:mo><m:mi>h</m:mi><m:mo>)</m:mo></m:mrow><m:mo>|</m:mo><m:mo>≤</m:mo><m:mi>γ</m:mi></m:mrow></m:mrow></m:math> to hold for all <m:math overflow="scroll"><m:mrow><m:mi>h</m:mi><m:mo>∈</m:mo><m:mi mathvariant="script">H</m:mi></m:mrow></m:math>
(and hence <m:math overflow="scroll"><m:mrow><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:mover accent="true"><m:mi>h</m:mi><m:mo>^</m:mo></m:mover><m:mo>)</m:mo></m:mrow><m:mo>≤</m:mo><m:mi>ε</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>h</m:mi><m:mo>*</m:mo></m:msup><m:mo>)</m:mo></m:mrow><m:mo>+</m:mo><m:mn>2</m:mn><m:mi>γ</m:mi></m:mrow></m:math>) with probability at least <m:math overflow="scroll"><m:mrow><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>δ</m:mi></m:mrow></m:math>,
it suffices that <m:math overflow="scroll"><m:mrow><m:mi>m</m:mi><m:mo>=</m:mo><m:msub><m:mi>O</m:mi><m:mrow><m:mi>γ</m:mi><m:mo>,</m:mo><m:mi>δ</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:mi>d</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math>.</para>
      <para id="id138764">In other words, the number of training examples needed to learn “well” using <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math> is linear
in the VC dimension of <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>. It turns out that, for “most” hypothesis classes, the VC dimension
(assuming a “reasonable” parameterization) is also roughly linear in the number of parameters.
Putting these together, we conclude that (for an algorithm that
tries to minimize training error) the number of training
examples needed is usually roughly linear in the number of parameters of <m:math overflow="scroll"><m:mi mathvariant="script">H</m:mi></m:math>.</para>
    </section>
  </content>
</document>